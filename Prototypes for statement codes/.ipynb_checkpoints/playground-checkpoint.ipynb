{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f38625c-322c-45d7-8de5-9dbb1ec4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcf329-3b4e-4935-9257-5921e34f6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dis = 128\n",
    "batch_size = 4\n",
    "\n",
    "num_proto = 10\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "336d65cf-7af6-43fe-9b77-c4a51fbc90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_correct_class = torch.randn(batch_size, num_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aebb8fd-cdb7-4f01-95c1-521af1cb5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dis = torch.randn(batch_size, num_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "454cc9a0-7902-49c5-a55e-a39c3aa5f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = max_dis - min_dis\n",
    "r.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeffc310-6496-4802-ac17-f30c0271725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "v, ind = torch.max(r*p_correct_class, dim=1)\n",
    "print(v.shape, ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0a405b6-b182-4be8-838d-b9d13008e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1.]])\n",
      "Epoch 1/10, Loss: 0.7475\n",
      "Epoch 2/10, Loss: 0.7468\n",
      "Epoch 3/10, Loss: 0.7460\n",
      "Epoch 4/10, Loss: 0.7453\n",
      "Epoch 5/10, Loss: 0.7445\n",
      "Epoch 6/10, Loss: 0.7439\n",
      "Epoch 7/10, Loss: 0.7432\n",
      "Epoch 8/10, Loss: 0.7424\n",
      "Epoch 9/10, Loss: 0.7417\n",
      "Epoch 10/10, Loss: 0.7410\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # 10 input features, 5 output classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # Output logits\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = SimpleModel()\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid and binary cross-entropy loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create synthetic data\n",
    "num_samples = 100\n",
    "input_features = 10\n",
    "num_classes = 5\n",
    "\n",
    "# Random input features\n",
    "inputs = torch.randn(num_samples, input_features)\n",
    "# Random targets (binary labels)\n",
    "targets = torch.randint(0, 2, (num_samples, num_classes)).float()\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6585343d-325f-49bf-9417-a4d7cda0554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "indices=tensor([0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 1, 3, 0, 0, 3, 1, 2, 0, 1, 2, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 3, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0,\n",
       "        0, 1, 1, 1, 3, 0, 0, 0, 0, 0, 1, 2, 1, 1, 3, 4, 1, 0, 1, 0, 2, 4, 1, 0,\n",
       "        0, 0, 0, 2]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(targets, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b203e271-6ecc-44ce-96f9-27f66be10852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [ True,  True, False],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example logits from the model (batch_size x num_classes)\n",
    "logits = torch.tensor([[2.0, -1.0, 0.5],\n",
    "                       [0.1, 1.5, -0.2],\n",
    "                       [-0.5, -1.2, 0.3]])\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "probabilities = torch.sigmoid(logits)\n",
    "probabilities > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92cb53e2-fd6c-4b63-85d3-6919fd671276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9980e-07, 2.9980e-07, 2.9980e-07],\n",
      "        [2.9980e-07, 2.9980e-07, 2.9980e-07],\n",
      "        [2.9980e-07, 2.9980e-07, 2.9980e-07]])\n",
      "AUROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Example logits and one-hot encoded labels\n",
    "logits = torch.tensor([[2, -1.0, 0.5],\n",
    "                       [0.1, 1.5, -0.2],\n",
    "                       [-0.5, -1.2, 0.3]])\n",
    "labels = torch.tensor([[1, 0, 1],\n",
    "                       [0, 1, 0],\n",
    "                       [1, 1, 1]])\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "probabilities = torch.sigmoid(logits)\n",
    "print(probabilities)\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn\n",
    "probabilities_np = probabilities.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Compute AUROC for each class\n",
    "auroc = roc_auc_score(labels_np, probabilities_np, average='macro')\n",
    "\n",
    "print(\"AUROC:\", auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abd79a7c-d013-4713-b2cc-2f7663dd5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_codes_batch = torch.tensor([\n",
    "    [0,0,0,0,0,0,1,1], \n",
    "    [1,0,0,0,0,1,1,0], \n",
    "    [0,0,1,1,1,1,1,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3265eb42-ba6d-4691-af2b-8241e9212cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {}\n",
    "num_scp_codes = 71\n",
    "for i in range(num_scp_codes):\n",
    "    lookup[i] = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d3f4d08-4d9e-43e4-a538-0ed1a10d4ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7], [0, 5, 6], [2, 3, 4, 5, 6]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get indices the scp codes which are activated for each example\n",
    "non_zero_indices = [list(row.nonzero(as_tuple=True)[0].tolist()) for row in scp_codes_batch]\n",
    "non_zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9877752c-9c2c-4a9b-925d-a4b64e29945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_number, row in enumerate(non_zero_indices):\n",
    "    for scp_class in row:\n",
    "        lookup[scp_class] += [ex_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8afeecf-7eb0-4ca8-9aab-4fca7d7e736a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1],\n",
       " 1: [],\n",
       " 2: [2],\n",
       " 3: [2],\n",
       " 4: [2],\n",
       " 5: [1, 2],\n",
       " 6: [0, 1, 2],\n",
       " 7: [0],\n",
       " 8: [],\n",
       " 9: [],\n",
       " 10: [],\n",
       " 11: [],\n",
       " 12: [],\n",
       " 13: [],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [],\n",
       " 17: [],\n",
       " 18: [],\n",
       " 19: [],\n",
       " 20: [],\n",
       " 21: [],\n",
       " 22: [],\n",
       " 23: [],\n",
       " 24: [],\n",
       " 25: [],\n",
       " 26: [],\n",
       " 27: [],\n",
       " 28: [],\n",
       " 29: [],\n",
       " 30: [],\n",
       " 31: [],\n",
       " 32: [],\n",
       " 33: [],\n",
       " 34: [],\n",
       " 35: [],\n",
       " 36: [],\n",
       " 37: [],\n",
       " 38: [],\n",
       " 39: [],\n",
       " 40: [],\n",
       " 41: [],\n",
       " 42: [],\n",
       " 43: [],\n",
       " 44: [],\n",
       " 45: [],\n",
       " 46: [],\n",
       " 47: [],\n",
       " 48: [],\n",
       " 49: [],\n",
       " 50: [],\n",
       " 51: [],\n",
       " 52: [],\n",
       " 53: [],\n",
       " 54: [],\n",
       " 55: [],\n",
       " 56: [],\n",
       " 57: [],\n",
       " 58: [],\n",
       " 59: [],\n",
       " 60: [],\n",
       " 61: [],\n",
       " 62: [],\n",
       " 63: [],\n",
       " 64: [],\n",
       " 65: [],\n",
       " 66: [],\n",
       " 67: [],\n",
       " 68: [],\n",
       " 69: [],\n",
       " 70: []}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "77ca86db-3626-4386-a984-703a454f639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dbf609dd-2ca4-4331-b36b-b21050903af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2421bb04-0d20-48d6-842d-bf01463ac5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60151417-630b-48ca-ae98-60993a04d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all.extend(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8f063d82-4d6c-4464-b6ce-eeb47103c2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.2987537 ,  0.801322  , -1.426755  , -0.33992597, -0.21288614],\n",
       "       dtype=float32),\n",
       " array([-2.358338  , -1.0712556 , -1.588623  , -0.73121345,  1.0472441 ],\n",
       "       dtype=float32),\n",
       " array([-1.2987537 ,  0.801322  , -1.426755  , -0.33992597, -0.21288614],\n",
       "       dtype=float32),\n",
       " array([-2.358338  , -1.0712556 , -1.588623  , -0.73121345,  1.0472441 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "37807392-f3b5-4ae4-99c7-fbfa51f48b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel ROC AUC: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# y_true contains the true binary labels for each class\n",
    "y_true = np.array([\n",
    "    [1, 0, 1],  # Example 1: belongs to class 0 and 2\n",
    "    [0, 1, 0],  # Example 2: belongs to class 1\n",
    "    [1, 1, 0],  # Example 3: belongs to class 0 and 1\n",
    "    [0, 0, 1],  # Example 4: belongs to class 2\n",
    "    [1, 1, 1]   # Example 5: belongs to all classes\n",
    "])\n",
    "\n",
    "# y_score contains the predicted probabilities for each class\n",
    "y_score = np.array([\n",
    "    [0.0, 0.1, 0.8],  # Example 1: high probability for class 0 and 2\n",
    "    [0.2, 0.05, 0.1], # Example 2: high probability for class 1\n",
    "    [0.08, 0.07, 0.2],  # Example 3: high probability for class 0 and 1\n",
    "    [0.3, 0.2, 0.09],  # Example 4: high probability for class 2\n",
    "    [0.07, 0.06, 0.08]   # Example 5: high probability for all classes\n",
    "])\n",
    "\n",
    "# Compute the multilabel ROC AUC score\n",
    "roc_auc = roc_auc_score(y_true, y_score, average=\"macro\")\n",
    "print(f\"Multilabel ROC AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179b84fd-0ce7-478c-93c1-33f6b21bf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated data loader code\n",
    "from settings import train_information, test_information, data_dir, num_train_examples, num_test_examples\n",
    "from dataset_class import ECGImageDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define transformations\n",
    "img_size = 224  # or whatever size you want\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Function to create a subset of the dataset\n",
    "def create_subset(dataset, num_examples):\n",
    "    # Ensure num_examples doesn't exceed the dataset length\n",
    "    num_examples = min(len(dataset), num_examples)\n",
    "    indices = np.random.choice(len(dataset), num_examples, replace=False)\n",
    "    subset = torch.utils.data.Subset(dataset, indices)\n",
    "    return subset\n",
    "\n",
    "# Initialize dataset and dataloader for training\n",
    "train_dataset = ECGImageDataset(train_information, transform=transform)\n",
    "# Initialize dataset and dataloader for testing\n",
    "test_dataset = ECGImageDataset(test_information, transform=transform)\n",
    "\n",
    "if num_train_examples is not None:\n",
    "    train_subset = create_subset(train_dataset, num_train_examples)\n",
    "else: \n",
    "    train_subset = train_dataset\n",
    "\n",
    "if num_test_examples is not None:\n",
    "    test_subset = create_subset(test_dataset, num_test_examples)\n",
    "else:\n",
    "    test_subset = test_dataset\n",
    "\n",
    "# Create data loaders for the subsets\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457f7cf7-453b-4a36-bb79-78bdff429ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 71])\n",
      "tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (image, labels) in enumerate(test_loader):\n",
    "    print(i)\n",
    "    input = image.cuda() # (batch_size, 3, 224, 224)\n",
    "    print(image.shape)\n",
    "\n",
    "    scp_labels = labels[1].cuda() # (batch_size, 71)\n",
    "    label = labels[0]  # (batch_size, 5)\n",
    "    print(scp_labels.shape)\n",
    "    print(label)\n",
    "\n",
    "    target = label.cuda().float()  # (batch_size, num_classes) --> one hot labels for all examples\n",
    "    print(target)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "978ad7ce-0585-4318-ad8f-7662ced4e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original 2D tensor\n",
    "a = torch.tensor([\n",
    "    [0, 1, 0, 1],\n",
    "    [1, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Number of repetitions\n",
    "n = 3\n",
    "\n",
    "# Repeat every element of each row 'n' times\n",
    "repeated_a = a.repeat_interleave(n, dim=1)\n",
    "\n",
    "print(repeated_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2934ef8-c819-41ae-8924-3585206d2c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CD': 0, 'HYP': 1, 'MI': 2, 'NORM': 3, 'STTC': 4}\n",
      "{'1AVB': 0, '2AVB': 1, '3AVB': 2, 'ABQRS': 3, 'AFIB': 4, 'AFLT': 5, 'ALMI': 6, 'AMI': 7, 'ANEUR': 8, 'ASMI': 9, 'BIGU': 10, 'CLBBB': 11, 'CRBBB': 12, 'DIG': 13, 'EL': 14, 'HVOLT': 15, 'ILBBB': 16, 'ILMI': 17, 'IMI': 18, 'INJAL': 19, 'INJAS': 20, 'INJIL': 21, 'INJIN': 22, 'INJLA': 23, 'INVT': 24, 'IPLMI': 25, 'IPMI': 26, 'IRBBB': 27, 'ISCAL': 28, 'ISCAN': 29, 'ISCAS': 30, 'ISCIL': 31, 'ISCIN': 32, 'ISCLA': 33, 'ISC_': 34, 'IVCD': 35, 'LAFB': 36, 'LAO/LAE': 37, 'LMI': 38, 'LNGQT': 39, 'LOWT': 40, 'LPFB': 41, 'LPR': 42, 'LVH': 43, 'LVOLT': 44, 'NDT': 45, 'NORM': 46, 'NST_': 47, 'NT_': 48, 'PAC': 49, 'PACE': 50, 'PMI': 51, 'PRC(S)': 52, 'PSVT': 53, 'PVC': 54, 'QWAVE': 55, 'RAO/RAE': 56, 'RVH': 57, 'SARRH': 58, 'SBRAD': 59, 'SEHYP': 60, 'SR': 61, 'STACH': 62, 'STD_': 63, 'STE_': 64, 'SVARR': 65, 'SVTAC': 66, 'TAB_': 67, 'TRIGU': 68, 'VCLVH': 69, 'WPW': 70}\n"
     ]
    }
   ],
   "source": [
    "def load_mappings(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    mappings = {}\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.endswith('Mappings:'):\n",
    "            current_section = line.replace(' Mappings:', '')\n",
    "            mappings[current_section] = {}\n",
    "        elif ':' in line:\n",
    "            key, value = line.split(':')\n",
    "            mappings[current_section][key.strip()] = int(value.strip())\n",
    "\n",
    "    return mappings\n",
    "\n",
    "# Usage\n",
    "file_path = 'label_mappings.txt'  # Replace with your actual file path\n",
    "mappings = load_mappings(file_path)\n",
    "\n",
    "# Access the mappings\n",
    "diagnostic_superclass_mapping = mappings['Diagnostic Superclass']\n",
    "scp_labels_mapping = mappings['SCP Labels']\n",
    "\n",
    "print(diagnostic_superclass_mapping)\n",
    "print(scp_labels_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6323260b-7615-4bc2-b010-1e9df99cecb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1AVB',\n",
       " 1: '2AVB',\n",
       " 2: '3AVB',\n",
       " 3: 'ABQRS',\n",
       " 4: 'AFIB',\n",
       " 5: 'AFLT',\n",
       " 6: 'ALMI',\n",
       " 7: 'AMI',\n",
       " 8: 'ANEUR',\n",
       " 9: 'ASMI',\n",
       " 10: 'BIGU',\n",
       " 11: 'CLBBB',\n",
       " 12: 'CRBBB',\n",
       " 13: 'DIG',\n",
       " 14: 'EL',\n",
       " 15: 'HVOLT',\n",
       " 16: 'ILBBB',\n",
       " 17: 'ILMI',\n",
       " 18: 'IMI',\n",
       " 19: 'INJAL',\n",
       " 20: 'INJAS',\n",
       " 21: 'INJIL',\n",
       " 22: 'INJIN',\n",
       " 23: 'INJLA',\n",
       " 24: 'INVT',\n",
       " 25: 'IPLMI',\n",
       " 26: 'IPMI',\n",
       " 27: 'IRBBB',\n",
       " 28: 'ISCAL',\n",
       " 29: 'ISCAN',\n",
       " 30: 'ISCAS',\n",
       " 31: 'ISCIL',\n",
       " 32: 'ISCIN',\n",
       " 33: 'ISCLA',\n",
       " 34: 'ISC_',\n",
       " 35: 'IVCD',\n",
       " 36: 'LAFB',\n",
       " 37: 'LAO-or-LAE',\n",
       " 38: 'LMI',\n",
       " 39: 'LNGQT',\n",
       " 40: 'LOWT',\n",
       " 41: 'LPFB',\n",
       " 42: 'LPR',\n",
       " 43: 'LVH',\n",
       " 44: 'LVOLT',\n",
       " 45: 'NDT',\n",
       " 46: 'NORM',\n",
       " 47: 'NST_',\n",
       " 48: 'NT_',\n",
       " 49: 'PAC',\n",
       " 50: 'PACE',\n",
       " 51: 'PMI',\n",
       " 52: 'PRC(S)',\n",
       " 53: 'PSVT',\n",
       " 54: 'PVC',\n",
       " 55: 'QWAVE',\n",
       " 56: 'RAO-or-RAE',\n",
       " 57: 'RVH',\n",
       " 58: 'SARRH',\n",
       " 59: 'SBRAD',\n",
       " 60: 'SEHYP',\n",
       " 61: 'SR',\n",
       " 62: 'STACH',\n",
       " 63: 'STD_',\n",
       " 64: 'STE_',\n",
       " 65: 'SVARR',\n",
       " 66: 'SVTAC',\n",
       " 67: 'TAB_',\n",
       " 68: 'TRIGU',\n",
       " 69: 'VCLVH',\n",
       " 70: 'WPW'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scp_labels_mapping = {value:key.replace('/', '-or-') for key, value in scp_labels_mapping.items()}\n",
    "scp_labels_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67c6acf-ebd4-4669-9c77-c2e0566f9ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d427948-ad89-4a22-b4ee-68fcd8852505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecg_diagnosis_models)",
   "language": "python",
   "name": "ecg_diagnosis_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
