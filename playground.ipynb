{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517c6ae8-d44f-41c2-b6ed-624e807973b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Device count: 8\n",
      "Device name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905a34cf-3ca1-4a0d-99d0-bbe2b7d9d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0\n",
      "CUDA version: 12.1\n",
      "CUDA is available\n",
      "Device 0: Tesla V100-SXM2-32GB\n",
      "Device 1: Tesla V100-SXM2-32GB\n",
      "Device 2: Tesla V100-SXM2-32GB\n",
      "Device 3: Tesla V100-SXM2-32GB\n",
      "Device 4: Tesla V100-SXM2-32GB\n",
      "Device 5: Tesla V100-SXM2-32GB\n",
      "Device 6: Tesla V100-SXM2-32GB\n",
      "Device 7: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d4c81c-d4a9-4260-b676-af1d9011106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a32e45-2df5-4fda-add7-8946046e9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ecg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>filename_lr</th>\n",
       "      <th>filename_hr</th>\n",
       "      <th>Normal_ECG</th>\n",
       "      <th>ecg_lr_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>21312.0</td>\n",
       "      <td>records100/00000/00109_lr</td>\n",
       "      <td>records500/00000/00109_hr</td>\n",
       "      <td>True</td>\n",
       "      <td>00109_lr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19314</td>\n",
       "      <td>19314</td>\n",
       "      <td>19353</td>\n",
       "      <td>19389.0</td>\n",
       "      <td>records100/19000/19353_lr</td>\n",
       "      <td>records500/19000/19353_hr</td>\n",
       "      <td>False</td>\n",
       "      <td>19353_lr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12707</td>\n",
       "      <td>12707</td>\n",
       "      <td>12739</td>\n",
       "      <td>16579.0</td>\n",
       "      <td>records100/12000/12739_lr</td>\n",
       "      <td>records500/12000/12739_hr</td>\n",
       "      <td>True</td>\n",
       "      <td>12739_lr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18414</td>\n",
       "      <td>18414</td>\n",
       "      <td>18453</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>records100/18000/18453_lr</td>\n",
       "      <td>records500/18000/18453_hr</td>\n",
       "      <td>False</td>\n",
       "      <td>18453_lr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10879</td>\n",
       "      <td>10879</td>\n",
       "      <td>10906</td>\n",
       "      <td>14854.0</td>\n",
       "      <td>records100/10000/10906_lr</td>\n",
       "      <td>records500/10000/10906_hr</td>\n",
       "      <td>True</td>\n",
       "      <td>10906_lr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  index  Unnamed: 0  ecg_id  patient_id  \\\n",
       "0             0    108         108     109     21312.0   \n",
       "1             1  19314       19314   19353     19389.0   \n",
       "2             2  12707       12707   12739     16579.0   \n",
       "3             3  18414       18414   18453     21182.0   \n",
       "4             4  10879       10879   10906     14854.0   \n",
       "\n",
       "                 filename_lr                filename_hr  Normal_ECG  \\\n",
       "0  records100/00000/00109_lr  records500/00000/00109_hr        True   \n",
       "1  records100/19000/19353_lr  records500/19000/19353_hr       False   \n",
       "2  records100/12000/12739_lr  records500/12000/12739_hr        True   \n",
       "3  records100/18000/18453_lr  records500/18000/18453_hr       False   \n",
       "4  records100/10000/10906_lr  records500/10000/10906_hr        True   \n",
       "\n",
       "  ecg_lr_path  \n",
       "0    00109_lr  \n",
       "1    19353_lr  \n",
       "2    12739_lr  \n",
       "3    18453_lr  \n",
       "4    10906_lr  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_for_labels = '../../../data/padmalab_external/special_project/physionet.org/files/ptb-xl/1.0.3/ptbxl_train_label_df.csv'\n",
    "# Path to the image directory\n",
    "data_dir = '../../../data/padmalab_external/special_project/physionet.org/files/ptb-xl/1.0.3/records100_ground_truth'\n",
    "\n",
    "# Load the CSV file\n",
    "label_df = pd.read_csv(csv_file_for_labels)\n",
    "train_df = label_df.sample(frac = 0.8)\n",
    "test_df = label_df.drop(train_df.index)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6ff91f6-7814-4170-ab6d-591dbe36f540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12556"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0e0b62-4878-456d-9bae-aad38d17dec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3139"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd67f771-e507-43f7-be06-be3058178c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "img_size = 224  # or whatever size you want\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73976827-c005-4f06-8310-cd084b4d545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class ECGImageDataset(Dataset):\n",
    "    def __init__(self, label_df, image_dir, transform=None):\n",
    "        self.label_df = label_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        image_paths = []\n",
    "        for root, _, files in os.walk(self.image_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.png'):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "        return image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.label_df.iloc[idx]['ecg_lr_path'] + '-0.png'\n",
    "        matching_paths = [path for path in self.image_paths if img_name in path]\n",
    "        \n",
    "        # Use the first match if it exists\n",
    "        img_path = matching_paths[0] if matching_paths else None\n",
    "        \n",
    "        while img_path is None:\n",
    "            idx += 1\n",
    "            img_name = self.label_df.iloc[idx]['ecg_lr_path'] + '-0.png'\n",
    "            matching_paths = [path for path in self.image_paths if img_name in path]\n",
    "            # Use the first match if it exists\n",
    "            img_path = matching_paths[0] if matching_paths else None\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.label_df.iloc[idx]['Normal_ECG']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "train_dataset = ECGImageDataset(train_df, data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)\n",
    "\n",
    "test_dataset = ECGImageDataset(test_df, data_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee9d2f4-d942-44eb-bb3c-d0b547d949c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(labels.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc8f134-6d71-404d-bb6a-739bcd2e7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from helpers import makedir\n",
    "import model\n",
    "import push\n",
    "import train_and_test as tnt\n",
    "import save\n",
    "from log import create_logger\n",
    "from preprocess import mean, std, preprocess_input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e96f24-0c07-4b64-bee2-907fff09c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from helpers import list_of_distances, make_one_hot\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score,accuracy_score\n",
    "\n",
    "def _train_or_test(model, dataloader, optimizer=None, class_specific=True, use_l1_mask=True,\n",
    "                   coefs=None, log=print):\n",
    "    is_train = optimizer is not None\n",
    "    start = time.time()\n",
    "    n_examples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_cross_entropy = 0\n",
    "    total_cluster_cost = 0\n",
    "    total_separation_cost = 0\n",
    "    total_avg_separation_cost = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (image, label) in enumerate(dataloader):\n",
    "        if i > 3:\n",
    "            continue\n",
    "        input = image.cuda()\n",
    "        target = label.cuda()\n",
    "\n",
    "        grad_req = torch.enable_grad() if is_train else torch.no_grad()\n",
    "        with grad_req:\n",
    "            output, min_distances = model(input)\n",
    "\n",
    "            cross_entropy = torch.nn.functional.cross_entropy(output, target)\n",
    "\n",
    "            if class_specific:\n",
    "                max_dist = (model.module.prototype_shape[1]\n",
    "                            * model.module.prototype_shape[2]\n",
    "                            * model.module.prototype_shape[3])\n",
    "\n",
    "                prototypes_of_correct_class = torch.t(model.module.prototype_class_identity[:, label]).cuda()\n",
    "                inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
    "                cluster_cost = torch.mean(max_dist - inverted_distances)\n",
    "\n",
    "                prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
    "                inverted_distances_to_nontarget_prototypes, _ = \\\n",
    "                    torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
    "                separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
    "\n",
    "                avg_separation_cost = \\\n",
    "                    torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
    "                avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "                \n",
    "                if use_l1_mask:\n",
    "                    l1_mask = 1 - torch.t(model.module.prototype_class_identity).cuda()\n",
    "                    l1 = (model.module.last_layer.weight * l1_mask).norm(p=1)\n",
    "                else:\n",
    "                    l1 = model.module.last_layer.weight.norm(p=1)\n",
    "\n",
    "            else:\n",
    "                min_distance, _ = torch.min(min_distances, dim=1)\n",
    "                cluster_cost = torch.mean(min_distance)\n",
    "                l1 = model.module.last_layer.weight.norm(p=1)\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_examples += target.size(0)\n",
    "            n_batches += 1\n",
    "            n_correct += (predicted == target).sum().item()\n",
    "\n",
    "            total_cross_entropy += cross_entropy.item()\n",
    "            total_cluster_cost += cluster_cost.item()\n",
    "            total_separation_cost += separation_cost.item()\n",
    "            total_avg_separation_cost += avg_separation_cost.item()\n",
    "\n",
    "            # Append to all_labels and all_scores\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "            all_scores.extend(output.softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        if is_train:\n",
    "            if class_specific:\n",
    "                if coefs is not None:\n",
    "                    loss = (coefs['crs_ent'] * cross_entropy\n",
    "                          + coefs['clst'] * cluster_cost\n",
    "                          + coefs['sep'] * separation_cost\n",
    "                          + coefs['l1'] * l1)\n",
    "                else:\n",
    "                    loss = cross_entropy + 0.8 * cluster_cost - 0.08 * separation_cost + 1e-4 * l1\n",
    "            else:\n",
    "                if coefs is not None:\n",
    "                    loss = (coefs['crs_ent'] * cross_entropy\n",
    "                          + coefs['clst'] * cluster_cost\n",
    "                          + coefs['l1'] * l1)\n",
    "                else:\n",
    "                    loss = cross_entropy + 0.8 * cluster_cost + 1e-4 * l1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        del input\n",
    "        del target\n",
    "        del output\n",
    "        del predicted\n",
    "        del min_distances\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    log('\\ttime: \\t{0}'.format(end - start))\n",
    "    log('\\tcross ent: \\t{0}'.format(total_cross_entropy / n_batches))\n",
    "    log('\\tcluster: \\t{0}'.format(total_cluster_cost / n_batches))\n",
    "    if class_specific:\n",
    "        log('\\tseparation:\\t{0}'.format(total_separation_cost / n_batches))\n",
    "        log('\\tavg separation:\\t{0}'.format(total_avg_separation_cost / n_batches))\n",
    "    log('\\taccu: \\t\\t{0}%'.format(n_correct / n_examples * 100))\n",
    "    log('\\tl1: \\t\\t{0}'.format(model.module.last_layer.weight.norm(p=1).item()))\n",
    "    p = model.module.prototype_vectors.view(model.module.num_prototypes, -1).cpu()\n",
    "    with torch.no_grad():\n",
    "        p_avg_pair_dist = torch.mean(list_of_distances(p, p))\n",
    "    log('\\tp dist pair: \\t{0}'.format(p_avg_pair_dist.item()))\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, [score.argmax() for score in all_scores])\n",
    "    f1 = f1_score(all_labels, [score.argmax() for score in all_scores], average='weighted')\n",
    "    auroc = roc_auc_score(all_labels, all_scores[:, 0], multi_class='ovr', average='weighted')\n",
    "\n",
    "    log(f'\\tAccuracy: {accuracy * 100:.2f}%')\n",
    "    log(f'\\tF1 Score: {f1:.4f}')\n",
    "    log(f'\\tAUROC: {auroc:.4f}')\n",
    "\n",
    "    return n_correct / n_examples\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, class_specific=False, coefs=None, log=print):\n",
    "    assert(optimizer is not None)\n",
    "    \n",
    "    log('\\ttrain')\n",
    "    model.train()\n",
    "    return _train_or_test(model=model, dataloader=dataloader, optimizer=optimizer,\n",
    "                          class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "\n",
    "def test(model, dataloader, class_specific=False, log=print):\n",
    "    log('\\ttest')\n",
    "    model.eval()\n",
    "    return _train_or_test(model=model, dataloader=dataloader, optimizer=None,\n",
    "                          class_specific=class_specific, log=log)\n",
    "\n",
    "\n",
    "def last_only(model, log=print):\n",
    "    for p in model.module.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.module.add_on_layers.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.module.prototype_vectors.requires_grad = False\n",
    "    for p in model.module.last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    log('\\tlast layer')\n",
    "\n",
    "\n",
    "def warm_only(model, log=print):\n",
    "    for p in model.module.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.module.add_on_layers.parameters():\n",
    "        p.requires_grad = True\n",
    "    model.module.prototype_vectors.requires_grad = True\n",
    "    for p in model.module.last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    log('\\twarm')\n",
    "\n",
    "\n",
    "def joint(model, log=print):\n",
    "    for p in model.module.features.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in model.module.add_on_layers.parameters():\n",
    "        p.requires_grad = True\n",
    "    model.module.prototype_vectors.requires_grad = True\n",
    "    for p in model.module.last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    log('\\tjoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b83813-13be-4a62-a35e-c8eee9d6ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book keeping namings and code\n",
    "from settings import base_architecture, img_size, prototype_shape, num_classes, \\\n",
    "                     prototype_activation_function, add_on_layers_type, experiment_run\n",
    "\n",
    "base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n",
    "\n",
    "model_dir = './saved_models/' + base_architecture + '/' + experiment_run + '/'\n",
    "makedir(model_dir)\n",
    "# shutil.copy(src=os.path.join(os.getcwd(), __file__), dst=model_dir)\n",
    "shutil.copy(src=os.path.join(os.getcwd(), 'settings.py'), dst=model_dir)\n",
    "shutil.copy(src=os.path.join(os.getcwd(), base_architecture_type + '_features.py'), dst=model_dir)\n",
    "shutil.copy(src=os.path.join(os.getcwd(), 'model.py'), dst=model_dir)\n",
    "shutil.copy(src=os.path.join(os.getcwd(), 'train_and_test.py'), dst=model_dir)\n",
    "\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n",
    "img_dir = os.path.join(model_dir, 'img')\n",
    "makedir(img_dir)\n",
    "weight_matrix_filename = 'outputL_weights'\n",
    "prototype_img_filename_prefix = 'prototype-img'\n",
    "prototype_self_act_filename_prefix = 'prototype-self-act'\n",
    "proto_bound_boxes_filename_prefix = 'bb'\n",
    "\n",
    "# load the data\n",
    "from settings import train_dir, test_dir, train_push_dir, \\\n",
    "                     train_batch_size, test_batch_size, train_push_batch_size\n",
    "# ---------------------------------------------------------------\n",
    "# Updated data loader code\n",
    "from settings import data_dir, csv_file_for_labels\n",
    "from dataset_class import ECGImageDataset\n",
    "\n",
    "# Define transformations\n",
    "img_size = 224  # or whatever size you want\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Load the CSV file\n",
    "label_df = pd.read_csv(csv_file_for_labels)\n",
    "\n",
    "train_dataset = ECGImageDataset(label_df, data_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=False)\n",
    "\n",
    "# we should look into distributed sampler more carefully at torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "log('training set size: {0}'.format(len(train_loader.dataset)))\n",
    "# log('push set size: {0}'.format(len(train_push_loader.dataset)))\n",
    "# log('test set size: {0}'.format(len(test_loader.dataset)))\n",
    "log('batch size: {0}'.format(train_batch_size))\n",
    "\n",
    "# construct the model\n",
    "base_architecture = 'resnet18'\n",
    "ppnet = model.construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "#if prototype_activation_function == 'linear':\n",
    "#    ppnet.set_last_layer_incorrect_connection(incorrect_strength=0)\n",
    "ppnet = ppnet.to('cuda')\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "class_specific = True\n",
    "\n",
    "# define optimizer\n",
    "from settings import joint_optimizer_lrs, joint_lr_step_size\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.1)\n",
    "\n",
    "from settings import warm_optimizer_lrs\n",
    "warm_optimizer_specs = \\\n",
    "[{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "from settings import last_layer_optimizer_lr\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
    "\n",
    "# weighting of different training losses\n",
    "from settings import coefs\n",
    "\n",
    "# number of training epochs, number of warm epochs, push start epoch, push epochs\n",
    "from settings import num_train_epochs, num_warm_epochs, push_start, push_epochs\n",
    "\n",
    "# train the model\n",
    "log('start training')\n",
    "import copy\n",
    "for epoch in range(num_train_epochs):\n",
    "    log('epoch: \\t{0}'.format(epoch))\n",
    "\n",
    "    if epoch < num_warm_epochs:\n",
    "        tnt.warm_only(model=ppnet_multi, log=log)\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=warm_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "    else:\n",
    "        tnt.joint(model=ppnet_multi, log=log)\n",
    "        joint_lr_scheduler.step()\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=joint_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "    accu = tnt.test(model=ppnet_multi, dataloader=train_loader,  # CHANGE TEST_LOADER TO TRAIN_LOADER\n",
    "                    class_specific=class_specific, log=log)\n",
    "    save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n",
    "                                target_accu=0.70, log=log)\n",
    "\n",
    "    if epoch >= push_start and epoch in push_epochs:\n",
    "        push.push_prototypes(\n",
    "            train_loader, # pytorch dataloader (must be unnormalized in [0,1])   # CHANGE TRAIN_PUSH_LOADER TO TRAIN_LOADER\n",
    "            prototype_network_parallel=ppnet_multi, # pytorch network with prototype_vectors\n",
    "            class_specific=class_specific,\n",
    "            preprocess_input_function=preprocess_input_function, # normalize if needed\n",
    "            prototype_layer_stride=1,\n",
    "            root_dir_for_saving_prototypes=img_dir, # if not None, prototypes will be saved here\n",
    "            epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n",
    "            prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
    "            prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
    "            proto_bound_boxes_filename_prefix=proto_bound_boxes_filename_prefix,\n",
    "            save_prototype_class_identity=True,\n",
    "            log=log)\n",
    "        accu = tnt.test(model=ppnet_multi, dataloader=train_loader,  # CHANGE TEST_LOADER TO TRAIN_LOADER\n",
    "                        class_specific=class_specific, log=log)\n",
    "        save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'push', accu=accu,\n",
    "                                    target_accu=0.70, log=log)\n",
    " \n",
    "        if prototype_activation_function != 'linear':\n",
    "            tnt.last_only(model=ppnet_multi, log=log)\n",
    "            for i in range(20):\n",
    "                log('iteration: \\t{0}'.format(i))\n",
    "                _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n",
    "                              class_specific=class_specific, coefs=coefs, log=log)\n",
    "                accu = tnt.test(model=ppnet_multi, dataloader=train_loader, # CHANGE TEST_LOADER TO TRAIN_LOADER\n",
    "                                class_specific=class_specific, log=log)\n",
    "                save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n",
    "                                            target_accu=0.70, log=log)\n",
    "   \n",
    "logclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92179a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test message\n"
     ]
    }
   ],
   "source": [
    "def log_to_file_and_console(message, logfile='results.txt'):\n",
    "    print(message)\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "log_to_file_and_console(\"Test message\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
